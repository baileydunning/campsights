name: Performance

permissions:
  pull-requests: write

on:
  pull_request:
    types:
    - opened
    - synchronize
    - reopened

jobs:
  server:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/server
    outputs:
      mean_latency: ${{ steps.extract-artillery.outputs.mean_latency }}
      min_latency: ${{ steps.extract-artillery.outputs.min_latency }}
      max_latency: ${{ steps.extract-artillery.outputs.max_latency }}
      p95_latency: ${{ steps.extract-artillery.outputs.p95_latency }}
      p99_latency: ${{ steps.extract-artillery.outputs.p99_latency }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install backend dependencies
      run: npm ci

    - name: Build backend
      run: npm run build

    - name: Start backend
      run: NODE_ENV=test npm start &

    - name: Wait for backend health endpoint
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:3000/health; then
            echo "Backend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Backend did not start in time"; exit 1

    - name: Install Artillery
      run: npm install -g artillery

    - name: Run Artillery load test
      id: artillery-run
      run: |
        printf 'config:\n  target: "http://localhost:3000"\n  phases:\n    - duration: 10\n      arrivalRate: 10\nscenarios:\n  - flow:\n      - get:\n          url: "/api/v1/campsites"\n' > artillery.yml
        echo "--- Artillery config ---"
        cat artillery.yml
        echo "--- Running Artillery ---"
        artillery run --output artillery-report.json artillery.yml
        echo "--- Artillery report.json (head) ---"
        head -40 artillery-report.json || cat artillery-report.json
        echo "--- Artillery report.json (size) ---"
        ls -lh artillery-report.json

    - name: Extract latency metrics from Artillery report
      id: extract-artillery
      run: |
        echo "--- jq Artillery metrics extraction ---"
        jq '.' artillery-report.json || echo "jq failed to parse artillery-report.json"
        mean_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].mean // empty' artillery-report.json)
        min_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].min // empty' artillery-report.json)
        max_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].max // empty' artillery-report.json)
        p95_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p95 // empty' artillery-report.json)
        p99_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p99 // empty' artillery-report.json)
        echo "mean_latency=$mean_latency" >> $GITHUB_OUTPUT
        echo "min_latency=$min_latency" >> $GITHUB_OUTPUT
        echo "max_latency=$max_latency" >> $GITHUB_OUTPUT
        echo "p95_latency=$p95_latency" >> $GITHUB_OUTPUT
        echo "p99_latency=$p99_latency" >> $GITHUB_OUTPUT
        echo "Extracted: mean=$mean_latency min=$min_latency max=$max_latency p95=$p95_latency p99=$p99_latency"

    - name: Post or update PR comment with backend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-backend
        message: |
          ## Backend Latency Metrics
          - Mean: ${{ steps.extract-artillery.outputs.mean_latency }} ms
          - Min: ${{ steps.extract-artillery.outputs.min_latency }} ms
          - Max: ${{ steps.extract-artillery.outputs.max_latency }} ms
          - 95th percentile: ${{ steps.extract-artillery.outputs.p95_latency }} ms
          - 99th percentile: ${{ steps.extract-artillery.outputs.p99_latency }} ms

  client:
    needs: server
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/client
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install frontend dependencies
      run: npm ci

    - name: Build frontend
      run: npm run build

    - name: Start frontend
      run: npm run preview &

    - name: Wait for frontend
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:4173; then
            echo "Frontend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Frontend did not start in time"; exit 1

    - name: Install Lighthouse CI
      run: |
        echo "--- Installing Lighthouse CI ---"
        npm install -g @lhci/cli

    - name: Run Lighthouse CI
      run: |
        echo "--- Running Lighthouse CI ---"
        lhci autorun --collect.url=http://localhost:4173 --upload.target=filesystem --upload.outputDir=lhci-report
        echo "--- LHCI report directory contents ---"
        ls -l lhci-report || echo "lhci-report not found"
        echo "--- LHCI manifest.json (head) ---"
        head -40 lhci-report/manifest.json || cat lhci-report/manifest.json

    - name: Extract Lighthouse scores
      id: extract-lhci
      run: |
        echo "--- jq LHCI metrics extraction ---"
        jq '.' lhci-report/manifest.json || echo "jq failed to parse manifest.json"
        # Find the representative run (isRepresentativeRun==true), fallback to first if not found
        rep_index=$(jq 'map(.isRepresentativeRun) | index(true) // 0' lhci-report/manifest.json)
        perf=$(jq -r ".[${rep_index}].summary.performance // empty" lhci-report/manifest.json)
        acc=$(jq -r ".[${rep_index}].summary.accessibility // empty" lhci-report/manifest.json)
        best=$(jq -r ".[${rep_index}].summary[\"best-practices\"] // empty" lhci-report/manifest.json)
        seo=$(jq -r ".[${rep_index}].summary.seo // empty" lhci-report/manifest.json)
        echo "perf=$perf" >> $GITHUB_OUTPUT
        echo "acc=$acc" >> $GITHUB_OUTPUT
        echo "best=$best" >> $GITHUB_OUTPUT
        echo "seo=$seo" >> $GITHUB_OUTPUT
        echo "Extracted: perf=$perf acc=$acc best=$best seo=$seo"

    - name: Post or update PR comment with frontend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-frontend
        message: |
          ## Lighthouse Scores
          - Performance: ${{ steps.extract-lhci.outputs.perf }}
          - Accessibility: ${{ steps.extract-lhci.outputs.acc }}
          - Best Practices: ${{ steps.extract-lhci.outputs.best }}
          - SEO: ${{ steps.extract-lhci.outputs.seo }}
