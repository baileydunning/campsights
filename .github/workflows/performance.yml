name: Performance

permissions:
  pull-requests: write

on:
  pull_request:
    types:
    - opened
    - synchronize
    - reopened

jobs:
  server:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/server
    outputs:
      mean_latency: ${{ steps.extract-artillery.outputs.mean_latency }}
      min_latency: ${{ steps.extract-artillery.outputs.min_latency }}
      max_latency: ${{ steps.extract-artillery.outputs.max_latency }}
      p95_latency: ${{ steps.extract-artillery.outputs.p95_latency }}
      p99_latency: ${{ steps.extract-artillery.outputs.p99_latency }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install backend dependencies
      run: npm ci

    - name: Build backend
      run: npm run build

    - name: Start backend
      run: NODE_ENV=test npm start &

    - name: Wait for backend health endpoint
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:3000/health; then
            echo "Backend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Backend did not start in time"; exit 1

    - name: Install Artillery
      run: npm install -g artillery

    - name: Run Artillery load test
      id: artillery-run
      run: |
        printf 'config:\n  target: "http://localhost:3000"\n  phases:\n    - duration: 10\n      arrivalRate: 10\nscenarios:\n  - flow:\n      - get:\n          url: "/api/v1/campsites"\n' > artillery.yml
        artillery run --output artillery-report.json artillery.yml

    - name: Extract latency metrics from Artillery report
      id: extract-artillery
      run: |
        mean_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].mean // empty' artillery-report.json)
        min_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].min // empty' artillery-report.json)
        max_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].max // empty' artillery-report.json)
        p95_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p95 // empty' artillery-report.json)
        p99_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p99 // empty' artillery-report.json)
        echo "mean_latency=$mean_latency" >> $GITHUB_OUTPUT
        echo "min_latency=$min_latency" >> $GITHUB_OUTPUT
        echo "max_latency=$max_latency" >> $GITHUB_OUTPUT
        echo "p95_latency=$p95_latency" >> $GITHUB_OUTPUT
        echo "p99_latency=$p99_latency" >> $GITHUB_OUTPUT

    - name: Post or update PR comment with backend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-backend
        message: |
          ### Backend Latency Metrics
          - name: Post or update PR comment with backend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-backend
        message: |
          ## Backend Latency Metrics

          - Mean: ${{ steps.extract-artillery.outputs.mean_latency }} ms \
            ${{ if lt( steps.extract-artillery.outputs.mean_latency, '1' ) }}Excellent${{ 
               else if lt( steps.extract-artillery.outputs.mean_latency, '2' ) }}Good${{ 
               else }}Needs Improvement${{ end }}

          - Min: ${{ steps.extract-artillery.outputs.min_latency }} ms \
            ${{ if eq( steps.extract-artillery.outputs.min_latency, '0' ) }}Perfect${{ 
               else }}Check Minimum${{ end }}

          - Max: ${{ steps.extract-artillery.outputs.max_latency }} ms \
            ${{ if lt( steps.extract-artillery.outputs.max_latency, '5' ) }}Excellent${{ 
               else if lt( steps.extract-artillery.outputs.max_latency, '10' ) }}Good${{ 
               else }}Needs Attention${{ end }}

          - 95th percentile: ${{ steps.extract-artillery.outputs.p95_latency }} ms \
            ${{ if lt( steps.extract-artillery.outputs.p95_latency, '2' ) }}Good${{ 
               else }}Needs Improvement${{ end }}

          - 99th percentile: ${{ steps.extract-artillery.outputs.p99_latency }} ms \
            ${{ if lt( steps.extract-artillery.outputs.p99_latency, '3' ) }}Good${{ 
               else }}Needs Improvement${{ end }}

  client:
    needs: server
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/client
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install frontend dependencies
      run: npm ci

    - name: Build frontend
      run: npm run build

    - name: Start frontend
      run: npm run preview &

    - name: Wait for frontend
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:4173; then
            echo "Frontend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Frontend did not start in time"; exit 1

    - name: Install Lighthouse CI
      run: npm install -g @lhci/cli

    - name: Run Lighthouse CI
      run: lhci autorun --collect.url=http://localhost:4173 --upload.target=filesystem --upload.outputDir=lhci-report

    - name: Extract Lighthouse scores
      id: extract-lhci
      run: |
        # Find the representative run (isRepresentativeRun==true), fallback to first if not found
        rep_index=$(jq 'map(.isRepresentativeRun) | index(true) // 0' lhci-report/manifest.json)
        perf=$(jq -r ".[${rep_index}].summary.performance // empty" lhci-report/manifest.json)
        acc=$(jq -r ".[${rep_index}].summary.accessibility // empty" lhci-report/manifest.json)
        best=$(jq -r ".[${rep_index}].summary[\"best-practices\"] // empty" lhci-report/manifest.json)
        seo=$(jq -r ".[${rep_index}].summary.seo // empty" lhci-report/manifest.json)
        echo "perf=$perf" >> $GITHUB_OUTPUT
        echo "acc=$acc" >> $GITHUB_OUTPUT
        echo "best=$best" >> $GITHUB_OUTPUT
        echo "seo=$seo" >> $GITHUB_OUTPUT

        - name: Post or update PR comment with frontend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-frontend
        message: |
          ### Frontend Performance Metrics

          - Performance: ${{ steps.extract-lhci.outputs.perf }} \
            ${{ if gt( steps.extract-lhci.outputs.perf, '0.90' ) }}Excellent${{ 
               else if gt( steps.extract-lhci.outputs.perf, '0.50' ) }}Needs Improvement${{ 
               else }}Poor${{ end }}

          - Accessibility: ${{ steps.extract-lhci.outputs.acc }} \
            ${{ if eq( steps.extract-lhci.outputs.acc, '1.00' ) }}Perfect${{ 
               else }}Review Accessibility${{ end }}

          - Best Practices: ${{ steps.extract-lhci.outputs.best }} \
            ${{ if gt( steps.extract-lhci.outputs.best, '0.90' ) }}Good${{ 
               else }}Check Best Practices${{ end }}

          - SEO: ${{ steps.extract-lhci.outputs.seo }} \
            ${{ if gt( steps.extract-lhci.outputs.seo, '0.90' ) }}Strong${{ 
               else if gt( steps.extract-lhci.outputs.seo, '0.75' ) }}Could Improve${{ 
               else }}Needs Work${{ end }}

