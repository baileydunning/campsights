name: Performance

permissions:
  pull-requests: write

on:
  pull_request:
    types:
    - opened
    - synchronize
    - reopened

jobs:
  server:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/server
    outputs:
      mean_latency: ${{ steps.extract-artillery.outputs.mean_latency }}
      min_latency: ${{ steps.extract-artillery.outputs.min_latency }}
      max_latency: ${{ steps.extract-artillery.outputs.max_latency }}
      p95_latency: ${{ steps.extract-artillery.outputs.p95_latency }}
      p99_latency: ${{ steps.extract-artillery.outputs.p99_latency }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install backend dependencies
      run: npm ci

    - name: Build backend
      run: npm run build

    - name: Start backend
      run: NODE_ENV=test npm start &

    - name: Wait for backend health endpoint
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:3000/health; then
            echo "Backend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Backend did not start in time"; exit 1

    - name: Install Artillery
      run: npm install -g artillery

    - name: Run Artillery load test
      id: artillery-run
      run: |
        printf 'config:\n  target: "http://localhost:3000"\n  phases:\n    - duration: 10\n      arrivalRate: 10\nscenarios:\n  - flow:\n      - get:\n          url: "/api/v1/campsites"\n' > artillery.yml
        artillery run --output artillery-report.json artillery.yml

    - name: Extract latency metrics from Artillery report
      id: extract-artillery
      run: |
        mean_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].mean // empty' artillery-report.json)
        min_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].min // empty' artillery-report.json)
        max_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].max // empty' artillery-report.json)
        p95_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p95 // empty' artillery-report.json)
        p99_latency=$(jq -r '.aggregate["summaries"]["http.response_time"].p99 // empty' artillery-report.json)
        echo "mean_latency=$mean_latency" >> $GITHUB_OUTPUT
        echo "min_latency=$min_latency" >> $GITHUB_OUTPUT
        echo "max_latency=$max_latency" >> $GITHUB_OUTPUT
        echo "p95_latency=$p95_latency" >> $GITHUB_OUTPUT
        echo "p99_latency=$p99_latency" >> $GITHUB_OUTPUT

    - name: Compute backend metric statuses
      id: backend-status
      run: |
        mean=${{ steps.extract-artillery.outputs.mean_latency }}
        min=${{ steps.extract-artillery.outputs.min_latency }}
        max=${{ steps.extract-artillery.outputs.max_latency }}
        p95=${{ steps.extract-artillery.outputs.p95_latency }}
        p99=${{ steps.extract-artillery.outputs.p99_latency }}
        # Mean
        if (( $(echo "$mean < 1" | bc -l) )); then mean_status="Excellent";
        elif (( $(echo "$mean < 2" | bc -l) )); then mean_status="Good";
        else mean_status="Needs Improvement"; fi
        # Min
        if (( $(echo "$min == 0" | bc -l) )); then min_status="Perfect";
        else min_status="Check Minimum"; fi
        # Max
        if (( $(echo "$max < 5" | bc -l) )); then max_status="Excellent";
        elif (( $(echo "$max < 10" | bc -l) )); then max_status="Good";
        else max_status="Needs Attention"; fi
        # p95
        if (( $(echo "$p95 < 2" | bc -l) )); then p95_status="Good";
        else p95_status="Needs Improvement"; fi
        # p99
        if (( $(echo "$p99 < 3" | bc -l) )); then p99_status="Good";
        else p99_status="Needs Improvement"; fi
        echo "mean_status=$mean_status" >> $GITHUB_OUTPUT
        echo "min_status=$min_status" >> $GITHUB_OUTPUT
        echo "max_status=$max_status" >> $GITHUB_OUTPUT
        echo "p95_status=$p95_status" >> $GITHUB_OUTPUT
        echo "p99_status=$p99_status" >> $GITHUB_OUTPUT

    - name: Post or update PR comment with backend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-backend
        message: |
          ### Backend Latency Metrics

          - Mean: ${{ steps.extract-artillery.outputs.mean_latency }} ms ( ${{ steps.backend-status.outputs.mean_status }} )
          - Min: ${{ steps.extract-artillery.outputs.min_latency }} ms ( ${{ steps.backend-status.outputs.min_status }} )
          - Max: ${{ steps.extract-artillery.outputs.max_latency }} ms ( ${{ steps.backend-status.outputs.max_status }} )
          - p95: ${{ steps.extract-artillery.outputs.p95_latency }} ms ( ${{ steps.backend-status.outputs.p95_status }} )
          - p99: ${{ steps.extract-artillery.outputs.p99_latency }} ms ( ${{ steps.backend-status.outputs.p99_status }} )

  client:
    needs: server
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: packages/client
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Use Node.js 22
      uses: actions/setup-node@v4
      with:
        node-version: '22.x'

    - name: Install frontend dependencies
      run: npm ci

    - name: Build frontend
      run: npm run build

    - name: Start frontend
      run: npm run preview &

    - name: Wait for frontend
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:4173; then
            echo "Frontend is up!"; exit 0
          fi
          sleep 2
        done
        echo "Frontend did not start in time"; exit 1

    - name: Install Lighthouse CI
      run: npm install -g @lhci/cli

    - name: Run Lighthouse CI
      run: lhci autorun --collect.url=http://localhost:4173 --upload.target=filesystem --upload.outputDir=lhci-report

    - name: Extract Lighthouse scores
      id: extract-lhci
      run: |
        # Find the representative run (isRepresentativeRun==true), fallback to first if not found
        rep_index=$(jq 'map(.isRepresentativeRun) | index(true) // 0' lhci-report/manifest.json)
        perf=$(jq -r ".[${rep_index}].summary.performance // empty" lhci-report/manifest.json)
        acc=$(jq -r ".[${rep_index}].summary.accessibility // empty" lhci-report/manifest.json)
        best=$(jq -r ".[${rep_index}].summary[\"best-practices\"] // empty" lhci-report/manifest.json)
        seo=$(jq -r ".[${rep_index}].summary.seo // empty" lhci-report/manifest.json)
        echo "perf=$perf" >> $GITHUB_OUTPUT
        echo "acc=$acc" >> $GITHUB_OUTPUT
        echo "best=$best" >> $GITHUB_OUTPUT
        echo "seo=$seo" >> $GITHUB_OUTPUT

    - name: Compute frontend metric statuses
      id: frontend-status
      run: |
        perf=${{ steps.extract-lhci.outputs.perf }}
        acc=${{ steps.extract-lhci.outputs.acc }}
        best=${{ steps.extract-lhci.outputs.best }}
        seo=${{ steps.extract-lhci.outputs.seo }}
        # Performance
        if (( $(echo "$perf > 0.90" | bc -l) )); then perf_status="Excellent";
        elif (( $(echo "$perf > 0.50" | bc -l) )); then perf_status="Needs Improvement";
        else perf_status="Poor"; fi
        # Accessibility
        if (( $(echo "$acc == 1.00" | bc -l) )); then acc_status="Perfect";
        else acc_status="Review Accessibility"; fi
        # Best Practices
        if (( $(echo "$best > 0.90" | bc -l) )); then best_status="Good";
        else best_status="Check Best Practices"; fi
        # SEO
        if (( $(echo "$seo > 0.90" | bc -l) )); then seo_status="Strong";
        elif (( $(echo "$seo > 0.75" | bc -l) )); then seo_status="Could Improve";
        else seo_status="Needs Work"; fi
        echo "perf_status=$perf_status" >> $GITHUB_OUTPUT
        echo "acc_status=$acc_status" >> $GITHUB_OUTPUT
        echo "best_status=$best_status" >> $GITHUB_OUTPUT
        echo "seo_status=$seo_status" >> $GITHUB_OUTPUT

    - name: Post or update PR comment with frontend results
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: performance-frontend
        message: |
          ### Frontend Performance Metrics

          - Performance: ${{ steps.extract-lhci.outputs.perf }} ( ${{ steps.frontend-status.outputs.perf_status }} )
          - Accessibility: ${{ steps.extract-lhci.outputs.acc }} ( ${{ steps.frontend-status.outputs.acc_status }} )
          - Best Practices: ${{ steps.extract-lhci.outputs.best }} ( ${{ steps.frontend-status.outputs.best_status }} )
          - SEO: ${{ steps.extract-lhci.outputs.seo }} ( ${{ steps.frontend-status.outputs.seo_status }} )
